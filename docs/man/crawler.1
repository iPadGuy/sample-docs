.\" Man page generated from reStructuredText.
.
.TH "CRAWLER" "1" "Oct 28, 2019" "" "Crawler"
.SH NAME
crawler \- Crawler 1.1
.
.nr rst2man-indent-level 0
.
.de1 rstReportMargin
\\$1 \\n[an-margin]
level \\n[rst2man-indent-level]
level margin: \\n[rst2man-indent\\n[rst2man-indent-level]]
-
\\n[rst2man-indent0]
\\n[rst2man-indent1]
\\n[rst2man-indent2]
..
.de1 INDENT
.\" .rstReportMargin pre:
. RS \\$1
. nr rst2man-indent\\n[rst2man-indent-level] \\n[an-margin]
. nr rst2man-indent-level +1
.\" .rstReportMargin post:
..
.de UNINDENT
. RE
.\" indent \\n[an-margin]
.\" old: \\n[rst2man-indent\\n[rst2man-indent-level]]
.nr rst2man-indent-level -1
.\" new: \\n[rst2man-indent\\n[rst2man-indent-level]]
.in \\n[rst2man-indent\\n[rst2man-indent-level]]u
..
.SH INSTALLATION
.sp
At the command line:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
easy_install crawler
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Or, if you have virtualenvwrapper installed:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
mkvirtualenv crawler
pip install crawler
.ft P
.fi
.UNINDENT
.UNINDENT
.SH SUPPORT
.sp
The easiest way to get help with the project is to join the \fB#crawler\fP
channel on \fI\%Freenode\fP\&. We hang out there and you can get real\-time help with
your projects.  The other good way is to open an issue on \fI\%Github\fP\&.
.sp
The mailing list at \fI\%https://groups.google.com/forum/#!forum/crawler\fP is also available for support.
.SH COOKBOOK
.SS Crawl a web page
.sp
The most simple way to use our program is with no arguments.
Simply run:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
crawler <url>
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
to crawl a webpage.
.SS Crawl a page slowly
.sp
To add a delay to your crawler,
use \fB\-d\fP:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
crawler \-d 10 <url>
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
This will wait 10 seconds between page fetches.
.SS Crawl only your blog
.sp
You will want to use the \fB\-i\fP flag,
which while ignore URLs matching the passed regex:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
crawler \-i "^blog/" <url>
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
This will only crawl pages that contain your blog URL.
.INDENT 0.0
.INDENT 3.5
You will want to use the \fB\-i\fP flag,
which while ignore URLs matching the passed regex:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
crawler \-i "pdf$" <url>
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
This will ignore URLs that end in PDF.
.UNINDENT
.UNINDENT
.INDENT 0.0
.IP \(bu 2
genindex
.IP \(bu 2
modindex
.IP \(bu 2
search
.UNINDENT
.SH AUTHOR
iPad Guy
.SH COPYRIGHT
2019, iPad Guy
.\" Generated by docutils manpage writer.
.
